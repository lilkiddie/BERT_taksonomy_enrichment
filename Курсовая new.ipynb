{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForPreTraining\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from razdel import sentenize\n",
    "import nltk\n",
    "import py7zr\n",
    "from razdel import tokenize\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираю словарь ключ - слово, значение - список коретежей вида (номер параграфа, номер предложения, номер слова)\n",
    "vocabulary = {}\n",
    "cache = {}\n",
    "banned_tags = {\"NPRO\", \"PRED\", \"PREP\",\"CONJ\", \"PRCL\", \"INTJ\", \"PNCT\", \"UNKN\", \"NUMB\"}\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "idx = 0\n",
    "chunksize = 10 ** 5\n",
    "with pd.read_csv('news2017/1.csv', chunksize=chunksize) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk.dropna(inplace=True)\n",
    "        for i in tqdm(range(chunk.shape[0])):\n",
    "            string_to_parse = chunk.iloc[i].values[1]\n",
    "            string_to_parse = re.sub(' *\\n\\n *', '. ', string_to_parse)\n",
    "            new_str = re.sub(' +|\\n *\\n| *\\xa0| *\\n', ' ', string_to_parse)\n",
    "            sents = sentenize(new_str)\n",
    "            for j, sent in enumerate(sents):\n",
    "                words = tokenize(sent.text.lower())\n",
    "                for k, word in enumerate(words):\n",
    "                    w = word.text\n",
    "                    if w not in cache:\n",
    "                        pm_info = analyzer.parse(w)[0]\n",
    "                        cache[w] = pm_info\n",
    "                        w_syn = pm_info\n",
    "                    else:\n",
    "                        w_syn = cache[w]\n",
    "                    if str(w_syn.tag).split(',')[0] not in banned_tags:\n",
    "                        normal_form = w_syn.normal_form\n",
    "                        if normal_form not in stopwords:\n",
    "                            if normal_form not in vocabulary:\n",
    "                                vocabulary[normal_form] = []\n",
    "                            vocabulary[normal_form].append((idx, j, k))\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pd.read_csv(\"WS353-russian-sim.txt\")\n",
    "words = set(ws[\"Word1\"]) | set(ws[\"Word2\"])\n",
    "pm = pymorphy2.MorphAnalyzer()\n",
    "for word in words:\n",
    "    w = pm.parse(word.lower().strip())[0].normal_form\n",
    "    ws353_words[w] = vocabulary[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81492470",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ws353.pkl', 'wb') as f:\n",
    "    pickle.dump(ws353_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ws353.pkl', 'rb') as handle:\n",
    "    ws_353 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news2017/1.csv\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434564c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7400ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    return (x @ y) / (numpy.linalg.norm(x) * numpy.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a841cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors - словарь статических векторов\n",
    "n_contexts = 2000\n",
    "i = 1\n",
    "for key, value in ws_353.items():\n",
    "    print(f'Iteration {i} / {len(ws_353)}')\n",
    "    i += 1\n",
    "    vector = []\n",
    "    n = 0\n",
    "    for idx, n_sent, w_idx in tqdm(value):\n",
    "        if n >= n_contexts:\n",
    "            break\n",
    "        n += 1\n",
    "        string_to_parse = df.iloc[idx].values[1]\n",
    "        string_to_parse = re.sub(' *\\n\\n *', '. ', string_to_parse)\n",
    "        new_str = re.sub(' +|\\n *\\n| *\\xa0| *\\n', ' ', string_to_parse)\n",
    "        sent = list(sentenize(new_str))[n_sent]\n",
    "        d = tokenizer(sent.text, return_offsets_mapping=True, return_tensors='pt')\n",
    "        offset_mapping = d.pop('offset_mapping').detach().numpy()[0]\n",
    "        try:\n",
    "            sent_vect = model(**d)[0].detach().numpy()[0]\n",
    "            word = list(tokenize(sent.text.lower()))[w_idx]\n",
    "            start = 1\n",
    "            while start < len(offset_mapping) - 1 and offset_mapping[start][0] != word.start:\n",
    "                start += 1\n",
    "            stop = start\n",
    "            while stop < len(offset_mapping) - 1 and offset_mapping[stop][1] != word.stop:\n",
    "                stop += 1\n",
    "            vector.append(sent_vect[start:stop+1].mean(axis=0))\n",
    "        except:\n",
    "            continue\n",
    "    vectors[key] = numpy.mean(vector, axis=0)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
