{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pymorphy2\n!pip install razdel","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:16:12.259985Z","iopub.execute_input":"2023-03-29T08:16:12.260304Z","iopub.status.idle":"2023-03-29T08:16:36.947612Z","shell.execute_reply.started":"2023-03-29T08:16:12.260272Z","shell.execute_reply":"2023-03-29T08:16:36.946197Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\nCollecting dawg-python>=0.7.1\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting razdel\n  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\nInstalling collected packages: razdel\nSuccessfully installed razdel-0.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\nimport pymorphy2\nfrom razdel import sentenize\nfrom razdel import tokenize\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport re\nimport gc\nimport tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-29T08:16:36.950830Z","iopub.execute_input":"2023-03-29T08:16:36.953519Z","iopub.status.idle":"2023-03-29T08:16:48.257118Z","shell.execute_reply.started":"2023-03-29T08:16:36.953479Z","shell.execute_reply":"2023-03-29T08:16:48.255987Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\nmodel = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\", output_hidden_states = True)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:16:48.258673Z","iopub.execute_input":"2023-03-29T08:16:48.259432Z","iopub.status.idle":"2023-03-29T08:16:51.874707Z","shell.execute_reply.started":"2023-03-29T08:16:48.259391Z","shell.execute_reply":"2023-03-29T08:16:51.873447Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/341 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5b2890747f47c5a4594e17c9eda288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad29908b6e94938bc59c58d87257796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/241k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b85478c48874b1c9c9eb9e3db1faca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/468k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c25a6e48df4ceb87878de04045dd93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76cea522310445fcb6e71568386294da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/47.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb1712b04714db6a3c833477447d9cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(29564, 312, padding_idx=0)\n    (position_embeddings): Embedding(512, 312)\n    (token_type_embeddings): Embedding(2, 312)\n    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=312, out_features=312, bias=True)\n            (key): Linear(in_features=312, out_features=312, bias=True)\n            (value): Linear(in_features=312, out_features=312, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=312, out_features=312, bias=True)\n            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=312, out_features=600, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=600, out_features=312, bias=True)\n          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=312, out_features=312, bias=True)\n            (key): Linear(in_features=312, out_features=312, bias=True)\n            (value): Linear(in_features=312, out_features=312, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=312, out_features=312, bias=True)\n            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=312, out_features=600, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=600, out_features=312, bias=True)\n          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=312, out_features=312, bias=True)\n            (key): Linear(in_features=312, out_features=312, bias=True)\n            (value): Linear(in_features=312, out_features=312, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=312, out_features=312, bias=True)\n            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=312, out_features=600, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=600, out_features=312, bias=True)\n          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=312, out_features=312, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states = True)\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Проблема с этой моделью**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n\nmodel = AutoModel.from_pretrained(\"sberbank-ai/ruRoberta-large\", output_hidden_states=True)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:39:23.062468Z","iopub.execute_input":"2023-03-29T09:39:23.063194Z","iopub.status.idle":"2023-03-29T09:39:53.210427Z","shell.execute_reply.started":"2023-03-29T09:39:23.063157Z","shell.execute_reply":"2023-03-29T09:39:53.209415Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0533586a0ec64e1ab3c97dbe70ac6665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad88f3de2b249a98bc8a424222ddfcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba6e75ee0144c989818827561b99bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6353f89bc0347008839f304624e2b05"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/newsdataset1csv/1.csv\")\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:16:51.877967Z","iopub.execute_input":"2023-03-29T08:16:51.878664Z","iopub.status.idle":"2023-03-29T08:17:33.555938Z","shell.execute_reply.started":"2023-03-29T08:16:51.878616Z","shell.execute_reply":"2023-03-29T08:17:33.554828Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vectors = {}","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:39:53.214867Z","iopub.execute_input":"2023-03-29T09:39:53.217455Z","iopub.status.idle":"2023-03-29T09:39:53.223666Z","shell.execute_reply.started":"2023-03-29T09:39:53.217402Z","shell.execute_reply":"2023-03-29T09:39:53.222483Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pm = pymorphy2.MorphAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:17:33.567942Z","iopub.execute_input":"2023-03-29T08:17:33.568585Z","iopub.status.idle":"2023-03-29T08:17:34.159105Z","shell.execute_reply.started":"2023-03-29T08:17:33.568547Z","shell.execute_reply":"2023-03-29T08:17:34.158044Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ws353_new = pd.read_csv(\"/kaggle/input/ws353-full/hj-wordsim353-all.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:17:34.161379Z","iopub.execute_input":"2023-03-29T08:17:34.162165Z","iopub.status.idle":"2023-03-29T08:17:34.172756Z","shell.execute_reply.started":"2023-03-29T08:17:34.162123Z","shell.execute_reply":"2023-03-29T08:17:34.171722Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Я предварительно собрал инвертированный индекс для набора слов из ws353, так как файл vocabulary слишком большой**","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/ws353-full/ws353_full.pkl', 'rb') as f:\n    ws_full = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:17:34.174255Z","iopub.execute_input":"2023-03-29T08:17:34.179119Z","iopub.status.idle":"2023-03-29T08:17:43.980478Z","shell.execute_reply.started":"2023-03-29T08:17:34.179078Z","shell.execute_reply":"2023-03-29T08:17:43.979409Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Проблема заключается в слудющем, я не понимаю как перекинуть вычисления на gpu. У меня в итоге вычислений после одной итерации vector is None, хотя там должны быть вектора, количество которых равно n_contexts. ","metadata":{}},{"cell_type":"code","source":"n_c = [50, 100, 200, 500, 1000, 2000]","metadata":{"execution":{"iopub.status.busy":"2023-03-29T07:28:38.030389Z","iopub.execute_input":"2023-03-29T07:28:38.030746Z","iopub.status.idle":"2023-03-29T07:28:38.036027Z","shell.execute_reply.started":"2023-03-29T07:28:38.030709Z","shell.execute_reply":"2023-03-29T07:28:38.034699Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from numpy.random import default_rng\nrng = default_rng()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:18:53.302261Z","iopub.execute_input":"2023-03-29T08:18:53.302642Z","iopub.status.idle":"2023-03-29T08:18:53.308407Z","shell.execute_reply.started":"2023-03-29T08:18:53.302608Z","shell.execute_reply":"2023-03-29T08:18:53.307245Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"contexts = {}\nvectors = {}","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:40:01.559344Z","iopub.execute_input":"2023-03-29T09:40:01.559935Z","iopub.status.idle":"2023-03-29T09:40:01.565174Z","shell.execute_reply.started":"2023-03-29T09:40:01.559898Z","shell.execute_reply":"2023-03-29T09:40:01.563752Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# **Запускать эту ячейку**","metadata":{}},{"cell_type":"code","source":"model.to('cuda')\n# for n_contexts in n_c:\n#     if n_contexts == 1000:\n#         break\nn_contexts = 2000\nprint(f'Number of contexts = {n_contexts}')\ni = 1\n\nfor key, value in ws_full.items():\n    print(f'Iteration {i} / {len(ws_full)}')\n#     if i % 25 == 0:\n#         with open(f'n_{n_contexts}_{i // 25}.pkl', 'wb') as f:\n#             pickle.dump(contexts, f)\n#         del(contexts)\n#         gc.collect()\n#         contexts = {}\n    i += 1\n    \n    vector = None # здесь будут храниться эмбеддинги, то есть его размер будет (n_contexts, 312)\n    n = 0\n    size = n_contexts if n_contexts <= len(value) else len(value)\n    indices = rng.choice(len(value), size, replace=False)\n    for index in tqdm.tqdm(indices):\n        idx, n_sent, w_idx = value[index]\n        if n >= n_contexts:\n            break\n        n += 1\n        string_to_parse = df.iloc[idx].values[1]\n        string_to_parse = re.sub(' *\\n\\n *', '. ', string_to_parse)\n        new_str = re.sub(' +|\\n *\\n| *\\xa0| *\\n', ' ', string_to_parse)\n        sent = list(sentenize(new_str))[n_sent]\n\n        with torch.no_grad():\n            d = tokenizer(sent.text, return_offsets_mapping=True, return_tensors='pt', return_attention_mask=False, return_token_type_ids=False)\n            offset_mapping = d.pop('offset_mapping')\n            input_ids = d['input_ids'].to('cuda')\n            try:\n                sent_vec = model(input_ids).hidden_states\n                layers = torch.tensor(np.array([i.to('cpu').numpy() for i in sent_vec[1:]]), dtype=torch.float32)\n                word = list(tokenize(sent.text.lower()))[w_idx]\n                start = np.where(offset_mapping[:, :, 0] == word.start)[1][0]\n                stop = np.where(offset_mapping[:, :, 1] == word.stop)[1][0]\n                if vector is None:\n                    vector = [layers[:, :, start:stop+1, :].mean(axis=2)]\n                else:\n                    vector.append(layers[:, :, start:stop+1, :].mean(axis=2))\n            except:\n                continue\n    vector = torch.stack(vector).mean(axis=0)\n    vectors[key] = vector\n# if (len(contexts)):\n#     with open(f'n_{n_contexts}_{i // 25}.pkl', 'wb') as f:\n#         pickle.dump(contexts, f)\n#     del(contexts)\n#     gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:40:08.526550Z","iopub.execute_input":"2023-03-29T09:40:08.527146Z","iopub.status.idle":"2023-03-29T09:57:59.025874Z","shell.execute_reply.started":"2023-03-29T09:40:08.527110Z","shell.execute_reply":"2023-03-29T09:57:59.024384Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Number of contexts = 2000\nIteration 1 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [01:00<00:00, 32.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:58<00:00, 34.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:58<00:00, 34.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [01:00<00:00, 33.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 8 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [00:14<00:00, 33.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 9 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [01:01<00:00, 32.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 10 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 11 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [01:00<00:00, 32.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 12 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:58<00:00, 34.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 13 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1482/1482 [00:46<00:00, 32.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 14 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:57<00:00, 34.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 15 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 16 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1090/1090 [00:32<00:00, 33.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 17 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:59<00:00, 33.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 18 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:57<00:00, 34.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 19 / 428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:57<00:00, 35.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 20 / 428\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▎     | 874/2000 [00:26<00:34, 32.51it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2598848876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moffset_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'offset_mapping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0msent_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.","output_type":"error"}]},{"cell_type":"code","source":"with open('vectors_2000_tiny.pkl', 'wb') as file:\n    pickle.dump(vectors, file)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:37:44.077319Z","iopub.execute_input":"2023-03-29T09:37:44.077759Z","iopub.status.idle":"2023-03-29T09:37:44.113801Z","shell.execute_reply.started":"2023-03-29T09:37:44.077718Z","shell.execute_reply":"2023-03-29T09:37:44.112802Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"score = ws353_new['sim'].values","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:00.453734Z","iopub.execute_input":"2023-03-29T09:38:00.454241Z","iopub.status.idle":"2023-03-29T09:38:00.469628Z","shell.execute_reply.started":"2023-03-29T09:38:00.454198Z","shell.execute_reply":"2023-03-29T09:38:00.468366Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"my_score = [[] for _ in range(3)]\nfor idx in range(ws353_new.shape[0]):\n    w1 = pm.parse(ws353_new.iloc[idx].word1)[0].normal_form\n    w2 = pm.parse(ws353_new.iloc[idx].word2)[0].normal_form\n    for j in range(3):\n        my_score[j].append(cosine_sim(vectors[w1][j], vectors[w2][j]).item())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:27.954710Z","iopub.execute_input":"2023-03-29T09:38:27.955677Z","iopub.status.idle":"2023-03-29T09:38:28.485621Z","shell.execute_reply.started":"2023-03-29T09:38:27.955629Z","shell.execute_reply":"2023-03-29T09:38:28.484532Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print([spearmanr(score, my_score[i]).correlation for i in range(3)])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:44.763484Z","iopub.execute_input":"2023-03-29T09:38:44.763970Z","iopub.status.idle":"2023-03-29T09:38:44.775560Z","shell.execute_reply.started":"2023-03-29T09:38:44.763934Z","shell.execute_reply":"2023-03-29T09:38:44.774228Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[0.49814499440208915, 0.5028291444279501, 0.5471547426905772]\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.figure(figsize=(14, 5))\nplt.plot([spearmanr(score, my_score[i]).correlation for i in range(3)])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:37.261957Z","iopub.execute_input":"2023-03-29T09:38:37.262675Z","iopub.status.idle":"2023-03-29T09:38:37.564348Z","shell.execute_reply.started":"2023-03-29T09:38:37.262598Z","shell.execute_reply":"2023-03-29T09:38:37.562843Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f8f0891fe50>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1400x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contexts['поражение'][0].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Этот кусок кода я использовал, чтобы на CPU произвести вычисления с маленькой моделью. Я решил посмотреть, как качество эмбеддингов зависит от слоя модели (здесь их всего 3). Качество на 200 контекстах получил в районе 0.57, что считаю очень хорошим результатом.","metadata":{}},{"cell_type":"code","source":"for n_contexts in n_c:\n    if n_contexts != 500:\n        continue\n    print(f'Number of contexts = {n_contexts}')\n    i = 1\n    contexts = {}\n    for key, value in ws_full.items():\n        print(f'Iteration {i} / {len(ws_full)}')\n        i += 1\n        vector = None # здесь будут храниться эмбеддинги, то есть его размер будет (n_contexts, 312)\n        n = 0\n        size = n_contexts if n_contexts <= len(value) else len(value)\n        indices = rng.choice(len(value), size, replace=False)\n        for index in tqdm.tqdm(indices):\n            idx, n_sent, w_idx = value[index]\n            if n >= n_contexts:\n                break\n            n += 1\n            string_to_parse = df.iloc[idx].values[1]\n            string_to_parse = re.sub(' *\\n\\n *', '. ', string_to_parse)\n            new_str = re.sub(' +|\\n *\\n| *\\xa0| *\\n', ' ', string_to_parse)\n            sent = list(sentenize(new_str))[n_sent]\n\n            with torch.no_grad():\n                d = tokenizer(sent.text, return_offsets_mapping=True, return_tensors='pt', return_attention_mask=False, return_token_type_ids=False)\n                offset_mapping = d.pop('offset_mapping')\n                input_ids = d['input_ids']\n                try:\n                    layers = torch.tensor(np.array([i.numpy() for i in list(model(input_ids).hidden_states)[1:]]), dtype=torch.float32)\n                    word = list(tokenize(sent.text.lower()))[w_idx]\n                    start = np.where(offset_mapping[:, :, 0] == word.start)[1][0]\n                    stop = np.where(offset_mapping[:, :, 1] == word.stop)[1][0]\n                    if vector is None:\n                        vector = [layers[:, :, start:stop+1:, ]]\n                    else:\n                        vector.append(layers[:, :, start:stop+1, :])\n                except:\n                    continue\n        contexts[key] = vector\n    vectors[n_contexts] = contexts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import product","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def g_pooling(collection, func, axis=0):\n    if func == torch.mean:\n        return torch.stack(tuple(func(vector[:, 0, ...], axis=axis) for vector in collection), dim=0)\n    return torch.stack(tuple(func(vector[:, 0, ...], axis=axis).values for vector in collection), dim=0)\ndef f_pooling(collection, func, axis=0):\n    if func == torch.mean:\n        return func(collection, axis=axis)\n    return func(collection, axis=axis).values\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"funcs = {'mean': torch.mean, 'min': torch.min, 'max': torch.max}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_500 ={}\nkeys = list(product(['mean', 'max', 'min'], repeat=2))\nfor key in tqdm.tqdm(keys):\n    f, g = key\n    stats = {}\n    print(key)\n    for word, lst in vectors[500].items():\n        stats[word] = f_pooling(g_pooling(lst, funcs[g], 1), funcs[f])\n    n_500[key] = stats\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('n_200.pkl', 'wb') as f:\n    pickle.dump(n_200, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_sim(x, y):\n    return (x @ y.T) / (np.linalg.norm(x) * np.linalg.norm(y))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:20.668443Z","iopub.execute_input":"2023-03-29T09:38:20.669148Z","iopub.status.idle":"2023-03-29T09:38:20.674254Z","shell.execute_reply.started":"2023-03-29T09:38:20.669111Z","shell.execute_reply":"2023-03-29T09:38:20.673077Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:38:20.879493Z","iopub.execute_input":"2023-03-29T09:38:20.879800Z","iopub.status.idle":"2023-03-29T09:38:21.073533Z","shell.execute_reply.started":"2023-03-29T09:38:20.879751Z","shell.execute_reply":"2023-03-29T09:38:21.072476Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"score = ws353_new['sim']\nres = {}\nfor key in keys:\n    my_score = [[], [], []]\n    for idx in range(ws353_new.shape[0]):\n        w1 = pm.parse(ws353_new.iloc[idx].word1)[0].normal_form\n        w2 = pm.parse(ws353_new.iloc[idx].word2)[0].normal_form\n        for i in range(3):\n            my_score[i].append(cosine_sim(n_200[(key[0], key[1])][w1][i], n_200[(key[0], key[1])][w2][i]))\n    res[key] = my_score\nplt.figure(figsize=(14, 7))\nplt.xticks(np.arange(0, 3))\nplt.grid()\nfor key in keys:\n    plt.plot(range(3), [spearmanr(res[key][i], score).correlation for i in range(3)], label=f'f={key[0]}, g={key[1]}')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = ws353_new['sim']\nres = {}\nfor key in keys:\n    my_score = [[], [], []]\n    for idx in range(ws353_new.shape[0]):\n        w1 = pm.parse(ws353_new.iloc[idx].word1)[0].normal_form\n        w2 = pm.parse(ws353_new.iloc[idx].word2)[0].normal_form\n        for i in range(3):\n            my_score[i].append(cosine_sim(n_100[(key[0], key[1])][w1][i], n_50[(key[0], key[1])][w2][i]))\n    res[key] = my_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[spearmanr(res[('mean','mean')][i], score).correlation for i in range(3)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 7))\nplt.xticks(np.arange(0, 3))\nplt.grid()\nfor key in keys:\n    plt.plot(range(3), [spearmanr(res[key][i], score).correlation for i in range(3)], label=f'f={key[0]}, g={key[1]}')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = ws353_new['sim']\nres = {}\nfor key in keys:\n    my_score = [[], [], []]\n    for idx in range(ws353_new.shape[0]):\n        w1 = pm.parse(ws353_new.iloc[idx].word1)[0].normal_form\n        w2 = pm.parse(ws353_new.iloc[idx].word2)[0].normal_form\n        for i in range(3):\n            my_score[i].append(cosine_sim(n_50[(key[0], key[1])][w1][i], n_50[(key[0], key[1])][w2][i]))\n    res[key] = my_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 7))\nplt.xticks(np.arange(0, 3))\nplt.grid()\nfor key in keys:\n    plt.plot(range(3), [spearmanr(res[key][i], score).correlation for i in range(3)], label=f'f={key[0]}, g={key[1]}')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}